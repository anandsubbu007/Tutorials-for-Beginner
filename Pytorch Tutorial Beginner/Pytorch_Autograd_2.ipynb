{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch Autograd - 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNaO1UerdPJmxfEv8NwUrmA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandsubbu007/Pytorch-Tutorial-Beginner/blob/master/Pytorch_Autograd_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRtUOuZjZ3EB",
        "colab_type": "text"
      },
      "source": [
        "# Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHpzsmE_Z4sg",
        "colab_type": "text"
      },
      "source": [
        "## Differential in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugMMV4-oxajU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Libraries\n",
        "import torch\n",
        "# Assign Device \n",
        "cuda0 = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXaU4tqjzxqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor([5],dtype=torch.float32,requires_grad=True)\n",
        "y = torch.tensor([6],dtype=torch.float32,requires_grad=True)\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E42KQxLqSbJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the function\n",
        "z = ((x**2)*y) + (x*y)\n",
        "print(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISsLr_WOS_o_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using autograd\n",
        "# Autograd to be applied on Scalars\n",
        "total = torch.sum(z) # Converting to scalar\n",
        "total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9yG8OInjA9G",
        "colab_type": "code",
        "outputId": "41fa6a0a-1842-4e44-a667-e73b53803ecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x.grad,y.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tzS-4N1TIkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total.backward() # to call grad function we need to call .backward() if not it will show as None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LayLMnsZYDnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Def with resp. to  x   :\",x.grad)\n",
        "print(\"Def with resp. to  y   :\",y.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUMQlRdlTiPT",
        "colab_type": "text"
      },
      "source": [
        "Finding Deferintial for x & y\n",
        "\n",
        "    z = (x^2)*y + xy\n",
        "      = (5^2 * 6) + 5*6\n",
        "      = 180\n",
        "\n",
        "    dz/dx = 2xy + y\n",
        "          = (2 * 5 * 6) + 6\n",
        "          = 66\n",
        "\n",
        "    dz/dy = x^2 + x\n",
        "          = 5^2 + 5\n",
        "          = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrJnd0A-Yz4o",
        "colab_type": "text"
      },
      "source": [
        "# Implementing Autograd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH30NAyta-Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randint(-100,100,(100,), dtype = torch.float32 , device = cuda0)\n",
        "y = (1.32*x) + 25                       # y = (w*x) + b     we are going to predict w & b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQdz9SiFbRdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = torch.ones(1,requires_grad = True, device = cuda0 )\n",
        "b = torch.ones(1,requires_grad = True, device = cuda0 ) \n",
        "y_hat = (w*x) + b\n",
        "\n",
        "epochs = 10000\n",
        "lr = 0.000001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruk19KAIgBS3",
        "colab_type": "code",
        "outputId": "7189d38b-d0b8-434c-e275-ef78e8a4042c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "count = 0\n",
        "for i in range(epochs):\n",
        "  loss = torch.sum((y_hat - y)**2) \n",
        "  loss.backward() \n",
        "  #w -= lr*w.grad --> this will be considered as relationship\n",
        "  with torch.no_grad(): # this will switch off gradients\n",
        "\n",
        "    w -= lr*w.grad\n",
        "    b -= lr*b.grad\n",
        "    count += 1\n",
        "    #setting gradients to be zero\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_() \n",
        "  \n",
        "  y_hat = (w*x ) + b\n",
        "\n",
        "print(count)\n",
        "print(\"Predicted w value  :\",w.item())\n",
        "print(\"Predicted b value  :\",b.item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "Predicted w value  : 1.3199260234832764\n",
            "Predicted b value  : 24.56040382385254\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}